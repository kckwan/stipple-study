<!DOCTYPE html>
<html>
	
    
    <head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">   
		<meta name="google-site-verification" content="1Nk-VQfCAo-lXKQHrwgqw8Aldjq2p5daCfYGFCRADWo" />
		<title>KC Kwan's Personal Webpage</title>
        <link rel="stylesheet" type="text/css" href="css/style.css">
		<link rel="shortcut icon" type="image/x-icon" href="#">
    </head>
    
    <body>
        <div class="home_main" id="kc">
			<div class="intro_box">
					<!---Hello! 你好! こにちは！Guten Tag!--->
					<div class="left">
						<div class="home_name">
								<p class = "myname"> 
									KWAN, Kin Chung (KC) <br>
									關健聰 かんけんそう</p>
								<p class = "mydegree">B.Sc., Ph.D (Computer Science, CUHK) </p>
						</div>
						<div class="home_intro">
								<p> 
									I am a researcher in Computer Graphics and a Ph.D. in Computer Science from the Chinese University of Hong Kong in 2015. My research experience is in the field of Non-Photorealistic computer graphics (NPR), VR and AR as well as human-computer interaction (HCI). I published multiple technical research papers in top conferences and journals such as SIGGRAPH (Asia), TVCG, CGF, and CHI. 
									<br><br>
									Currently I am working at <a class="better" href="https://www.uni-konstanz.de/en/" target="_blank">Univeristy of Konstanz</a> (Germany) within the DFG-funded collaborative research group "Quantitative Methods for Visual Computing" <a class="better" href="https://www.sfbtrr161.de/" target="_blank">(SFB-TRR 161)</a>.
								</p>
							</div>
					</div>
					<div class="right">
						<img src="img/photo/KC2.jpg"/>
					</div>
					<div class="mobile">
						<p style="text-align: center;font-size:4vw;margin-bottom: 0pt">
							<a class="better" href="https://www.uni-konstanz.de/en/" target="_blank">University of Konstanz</a>
						</p>
					</div>
			</div>
		</div>

        <div class ="header">
            <ul class="menubar">
                <li><a href="#kc">KC Kwan </a></li>
                <li><a href="#news_anchor">News</a></li>
                <li><a href="#profile_anchor">Profile</a></li>
                <li><a href="#paper_anchor">Papers</a></li>
                <li><a href="#activity_anchor">Activity</a></li>
<!--				<li><a href="#hobby_anchor">Hobby</a></li>-->
<!--				<li><a href="#resources_anchor">Resources</a></li>-->
                <li><a href="#link_anchor">Links</a></li>
                <!--<li><a href="#contact_anchor">Contact</a></li>-->
                <!-- <li style="float: right">Last update: 20 Apr 2020</li> -->
                <!-- <li style="float: right" id="back_button" ><a href="#" onclick="load_Paper_Page('home.html')">Back</a></li> -->
            </ul>
        </div>
	
		<div class="mobile">
			<center>
			<img style="width:50%;margin-top:50px;" src="img/photo/qrcode.png">
			</center>
		</div>


        <div class="content" style="width:100%">
			<a class="anchor" id="news_anchor"></a>
            <h1 id="news"><span> What's New</span></h1>
            <div class="section">
                <table class="news">
					<tr>
                        <td class="c1"><span class="newdatenew">Dec 2021</span></td>
                        <td class="c2">One paper has been accepted by Computational Visual Media (CVM) and recommended for publication in CVMJ.</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdatenew">Oct 2021</span></td>
                        <td class="c2">One short paper has been accepted by SIGGRAPH Asia 2021 technical communcition.</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdatenew">Sep 2021</span></td>
                        <td class="c2">Our inverted stippling paper has been accepted by SIGGRAPH Asia 2021.</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate2">Aug 2021</span></td>
                        <td class="c2">This webpage is online in github page now.</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate2">Dec 2020</span></td>
                        <td class="c2">Our AR Draw paper has been accepted by TVCG, and will be presented in IEEE VR 2021.</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate2">Dec 2020</span></td>
                        <td class="c2">Our checkpoint selection paper has been accepted by CGF.</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate2">Sep 2020</span></td>
                        <td class="c2">I moved to Konstanz, Germany! What a nice place!</td>
                    </tr>
                    <tr>
                        <td class="c1"><span class="newdate2">Apr 2020</span></td>
                        <td class="c2">I will join the University of Konstanz, Germany and the SFB-TRR 161.</td>
                    </tr>
                    <tr>
                        <td class="c1"><span class="newdate2">Mar 2020</span></td>
                        <td> Our ARAnimator paper has been conditionally accepted by SIGGRAPH 2020.</td>
                    </tr>
                </table>
                <div style="height:20pt"></div>
            </div>
            
            
            <a class="anchor" id="profile_anchor"></a>
            <h1 id="profile"><span> Profile</span></h1>
            <div class="section">
				<div class = "mobile">
					<h2 class="first">Who am I?</h2>
								<p style="text-align:justify;padding-right:10px"> 
									I am a researcher in Computer Graphics and a Ph.D. in Computer Science from the Chinese University of Hong Kong in 2015. My research experience is in the field of Non-Photorealistic computer graphics (NPR), VR and AR as well as human-computer interaction (HCI). I published multiple technical research papers in top conferences and journals such as SIGGRAPH (Asia), TVCG, CGF, and CHI. <BR>
									Currently I am working at <a class="better" href="https://www.uni-konstanz.de/en/" target="_blank">Univeristy of Konstanz</a> (Germany) within the DFG-funded collaborative  research group "Quantitative Methods for Visual Computing" <a class="better" href="https://www.sfbtrr161.de/" target="_blank">(SFB-TRR 161)</a>.
								</p>
				</div>
				<div style="height:50pt"></div>
                <h2 class="first">Academic Experiences</h2>
                <table class="edutable">
                    <tr>
                        <td class="c1"><span class="newdate">2020 - now</span></td>
                        <td class="c2"> 
						<div class="pc"><i><b>Postdoctoral Research Assistant</b></i> in University of Konstanz, Germany <a class="better" href="https://www.uni-konstanz.de/en" target="_blank">Uni-Konstanz</a> <br>
                        Supervisor: <a class="better" href="https://www.cgmi.uni-konstanz.de/en/persons/prof-dr-oliver-deussen/" target="_blank">Prof. Oliver Deussen </a></div>
						<div class="mobile"><i><b>Postdoctoral Research Assistant</b></i> <br> <a class="better" href="https://www.uni-konstanz.de/en" target="_blank">University of Konstanz</a></div>
                        </td>
                    </tr>                    
                    <tr>
                        <td class="c1"><span class="newdate">2018 - 2020</span></td>
                        <td class="c2"> 
							<div class ="pc"><i><b>Senior Research Assistant</b></i> in City University of Hong Kong, Hong Kong <a class="better" href="https://www.cityu.edu.hk/" target="_blank">CityU</a></div>
							<div class="mobile"><i><b>Senior Research Assistant</b></i> <br> <a class="better" href="https://www.cityu.edu.hk/" target="_blank">City University of Hong Kong</a></div>
						</td>
                    </tr>                    
                    <tr>
                        <td class="c1"><span class="newdate">2017 - 2018</span></td>
                        <td class="c2"> 
							<div class ="pc"><i><b>Postdoctoral Research Assistant</b></i> in City University of Hong Kong, Hong Kong <a class="better" href="https://www.cityu.edu.hk/" target="_blank">CityU</a> <br> Supervisor: <a class="better" href="http://sweb.cityu.edu.hk/hongbofu/" target="_blank">Prof. Hongbo Fu</a></div>
							<div class="mobile"><i><b>Postdoctoral Research Assistant</b></i> <br> <a class="better" href="https://www.cityu.edu.hk/" target="_blank">City University of Hong Kong</a></div>
                        </td>
                    </tr>                    
                    <tr>
                        <td class="c1"><span class="newdate">2015 - 2017</span></td>
                        <td class="c2"> 
							<div class ="pc"><i><b>Research Fellow</b></i> in Caritas Institute of Higher Education, Hong Kong <a class="better" href="https://www.cihe.edu.hk/" target="_blank">CIHE</a>  </div>
							<div class="mobile"><i><b>Research Fellow</b></i> <br> <a class="better" href="https://www.cihe.edu.hk/" target="_blank">Caritas Institute of Higher Education</a></div>
						</td>
							
                    </tr>
                    <tr>
                        <td class="c1"><span class="newdate">2014 - 2015</span></td>
                        <td class="c2"> 
							<div class ="pc"><i><b>Research Assistant</b></i> in Caritas Institute of Higher Education, Hong Kong <a class="better" href="https://www.cihe.edu.hk/" target="_blank">CIHE</a>  <br>Supervisor: <a class="better" href="http://www.raymond-pang.com/" target="_blank">Dr. Wai Man Pang (Raymond)</a></div>
							<div class="mobile"><i><b>Research Assistant</b></i> <br> <a class="better" href="https://www.cihe.edu.hk/" target="_blank">Caritas Institute of Higher Education</a></div>
                        </td>
                    </tr>
                    <tr>
                        <td class="c1"><span class="newdate">2013 - 2014</span></td>
                        <td class="c2"> 
							<div class ="pc"><i><b>Research Assistant</b></i> in Chinese University of Hong Kong, Hong Kong <a class="better" href="https://www.cuhk.edu.hk/" target="_blank">CUHK</a> </div>
							<div class="mobile"><i><b>Research Assistant</b></i> <br> <a class="better" href="https://www.cuhk.edu.hk/" target="_blank">Chinese University of Hong Kong</a></div>
						</td>
                    </tr>
                    <tr>
                        <td class="c1"><span class="newdate">2009 - 2015</span></td>
                        <td class="c2"> 
							<div class ="pc"><i><b>Ph.D.</b></i> in Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong <a class="better" href="https://www.cuhk.edu.hk/" target="_blank">CUHK</a> <br> Supervisor: <a class="better" href="https://www.cse.cuhk.edu.hk/~ttwong/" target="_blank">Prof. Tien-Tsin Wong (TT)</a> </div>
							<div class="mobile"><i><b>Ph.D.</b></i> in Computer Science and Engineering <br> <a class="better" href="https://www.cuhk.edu.hk/" target="_blank">Chinese University of Hong Kong</a>.</div>
                        </td>
                    </tr>
                    <tr>
                        <td class="c1"><span class="newdate">2006 - 2009</span></td>
                        <td class="c2">
							<div class ="pc"><i><b>B.Sc.</b></i> in Computer Science, Chinese University of Hong Kong, Hong Kong <a class="better" href="https://www.cuhk.edu.hk/" target="_blank">CUHK</a></div>
							<div class="mobile"><i><b>B.Sc.</b></i> in Computer Science <br> <a class="better" href="https://www.cuhk.edu.hk/" target="_blank">Chinese University of Hong Kong</a>.</div>
						</td>
                    </tr>
                
                </table>
                <div style="height:50pt"></div>
                <h2 class="other">My Information</h2>
                <table class="myinfotable">
					<tr>
						<td class="c1">Email:</td>
						<td> <img style="height:min(20px,3.5vw)" src="img/photo/email.png" /> </td>
					</tr>
					<tr>
						<td class="c1">Phone (DE):</td>
						<td> <img style="height:min(15px,2.5vw)" src="img/photo/de_phone_num.png" /> </td>
					</tr>
					<tr>
						<td class="c1">Phone (HK):</td>
						<td> <img style="height:min(15px,2.5vw)" src="img/photo/hk_phone_num.png" /></td>
					</tr>
					<tr>
						<td class="c1">Office:</td>
						<td>
						Room 804, 8/F,  Building ZT, University of Konstanz<br>
					</td>
					<tr>
						<td class="c1">Address:</td>
						<td>
						Universitätsstraße 10, 78457 Konstanz, Germany 
						<a class="better" href="https://goo.gl/maps/KUZm1Et745pguh9d9" target="_blank">Google Maps</a>
					</td>
					</tr>
					<tr>
						<td class="c1">My CV:</td>
						<td><a href="files/CV.pdf" target="_blank"><img class="icon" src="img/icon/PDF_32.png"/></a> Updated: Dec 2021</td>
					</tr>
					<tr>
						<td class="c1">My Resume:</td>
						<td><a href="files/Resume.pdf" target="_blank"><img class="icon" src="img/icon/PDF_32.png"/></a> Updated: Dec 2021</td>
					</tr>
					<tr>
						<td class="c1" >Google Scholar:</td>
						<td><a class="better" href="https://scholar.google.com/citations?hl=en&user=IxV7fuwAAAAJ" target="_blank">https://scholar.google.com/citations?hl=en&user=IxV7fuwAAAAJ</a></td>
					</tr>
					<tr>
						<td class="c1" >Linkedln:</td>
						<td><a class="better" href="https://www.linkedin.com/in/kckwan" target="_blank">www.linkedin.com/in/kckwan</a></td>
					</tr>
					<tr>
						<td class="c1" >Language:</td>
						<td>
						<table class="">
						<tr>
							<td>Cantonese 廣東話</td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td>Native</td>
						</tr>
						<tr>
							<td> Mandarin 普通话</td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td><div class="emptybox"></div> </td>
							<td>Fluent</td>
						</tr>
						<tr>
							<td>English</td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td><div class="emptybox"></div> </td>
							<td>Fluent</td>
						</tr>
						<tr>
							<td>Japanese 日本語</td>
							<td><div class="filledbox"></div></td>
							<td><div class="filledbox"></div></td>
							<td><div class="emptybox"></div></td>
							<td><div class="emptybox"></div> </td>
							<td><div class="emptybox"></div> </td>
							<td>Intermediate </td>
						</tr>
						<tr>
							<td>German Deutsch</td>
							<td><div class="filledbox"></div></td>
							<td><div class="emptybox"></div> </td>
							<td><div class="emptybox"></div> </td>
							<td><div class="emptybox"></div> </td>
							<td><div class="emptybox"></div> </td>
							<td>Beginner</td>
						</tr>
						</table>
						</td>
					</tr>
					
				</table>
             </div>
            
            <a class="anchor" id="paper_anchor"></a>
            <h1 id="paper"><span> Publications</span></h1>
			
            <div class="section">
				<h2>Selected List</h2>
				<center>
				<div class ="selectedpaperlist">
				<div class ="selectedpaper"><a href="#invertedstippling_anchor">
						<img src="img/rep/invertedstippling_rep.png"/><br>
						Inverted Stippling
					</a></div>
					<div class ="selectedpaper"><a href="#autocheckpoint_anchor">
						<img src="img/rep/autocheckpoint_rep.png"/><br>
						Image Checkpoint Selection
					</a></div>
					<div class ="selectedpaper"><a href="#ardraw_anchor">
						<img src="img/rep/ardraw_rep.png"/><BR>
						3D Curve Creation
					</a></div>
					<div class ="selectedpaper"><a href="#aranimator_anchor">
						<img src="img/rep/aranimator_rep.png"/><br>
						ARAnimator
					</a></div>
					<div class ="selectedpaper"><a href="#mobi_anchor">
						<img " src="img/rep/mobi_rep.png"/><br>
						Mobi3DSketch
					</a></div>
					<div class ="selectedpaper"><a href="#decomp_anchor">
						<img src="img/rep/decomp_rep.png"/><br>
						HW-decompressible Vertex
					</a></div>
					<div class ="selectedpaper"><a href="#pad_anchor">
						<img src="img/rep/pad_rep.png"/><br>
						Collage of Shapes
					</a></div>
				</div>
				</center>
			</div>
			<div class="section">
				<h2>Research Papers</h2>
                
                <table class="papertable">
					<tr>
						
                        <td class="c1"><img class="paperrep" src="img/rep/drawonreference_rep.png"/></td>
                        <td class="c2">
                            <snap class="papername">Autocomplete Repetitive Stroking with Image Guidance </snap>
							<snap class="paperconf">(SIGGRAPH Asia 2021 Technical Communications)</snap> <br>
                            <snap class="author">
							Yilan Chen, 
							<snap class="authorme">Kin Chung Kwan</snap>,
							Li-Yi Wei, Hongbo Fu
							</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
								Image-guided drawing can compensate for the lack of skills but often requires a significant number of repetitive strokes to create textures. Existing automatic stroke synthesis methods are usually limited to predefined styles or require indirect manipulation that may break the spontaneous flow of drawing. We present a method to autocomplete repetitive short strokes during users’ normal drawing process. Users can draw over a reference image as usual. At the same time, our system silently analyzes the input strokes and the reference to infer strokes that follow users’ input style when certain repetition is detected. Our key idea is to jointly analyze image regions and operation history for detecting and predicting repetitions. The proposed system can reduce tedious repetitive inputs while being fully under user control.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank" href="[2021_SATechComm]_Autocomplete_Repetitive_Stroking_with_Image_Guidance.pdf">Paper</a> 
							<a class="better" target="_blank" href="https://www.youtube.com/watch?v=_qtCsrrIJQo">Video</a>
							<a class="better" target="_blank" href="https://doi.org/10.1145/3478512.3488595">DOI</a>
							</div>
                        </td>
                    </tr>  
					<tr>
						
                        <td class="c1"><img class="paperrep" src="img/rep/invertedstippling_rep.png"/></td>
                        <td class="c2">
							<a class="anchor" id="invertedstippling_anchor"></a>
                            <snap class="papername">Multi-Class Inverted Stippling </snap>
							<snap class="paperconf">(SIGGRAPH Asia 2021)</snap> <br>
                            <snap class="author">
							Christoph Schulz, 
							<snap class="authorme">Kin Chung Kwan (Joint first author)</snap>,
							Michael Becher, Daniel Baumgartner, Guido Reina, Oliver Deussen, and Daniel Weiskopf
							</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
								We introduce inverted stippling, a method to mimic an inversion technique used by artists when performing stippling. To this end, we extend LindeBuzo-Gray (LBG) stippling to multi-class LBG (MLBG) stippling with multiple layers. MLBG stippling couples the layers stochastically to optimize for per-layer and overall blue-noise properties. We propose a stipple-based filling method to generate solid color backgrounds for inverting areas. Our experiments demonstrate the effectiveness of MLBG in terms of reducing overlapping and intensity accuracy. In addition, we showcase MLBG with color stippling and dynamic multi-class blue-noise sampling, which is possible due to its support for temporal coherence.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank" href="files/papers/[2021_SIGGRAPH_Asia]_Multi-Class_Inverted_Stippling.pdf">Paper</a> 
							<a class="better" target="_blank" href="[2021_SIGGRAPH_Asia]_Multi-Class_Inverted_Stippling_Supp">Supp</a>
							<a class="better" target="_blank" href="https://youtu.be/yEz96Nv2LJE">Video</a>
							<a class="better" target="_blank" href="https://github.com/UniStuttgart-VISUS/MLBGStippling">Code</a>
							<a class="better" target="_blank" href="https://doi.org/10.1145/3478513.3480534">DOI</a>
							</div>
                        </td>
                    </tr>  
					<tr>
						
                        <td class="c1"><img class="paperrep" src="img/rep/autocheckpoint_rep.png"/></td>
                        <td class="c2">
							<a class="anchor" id="autocheckpoint_anchor"></a>
                            <snap class="papername">Automatic Image Checkpoint Selection for Guider-Follower Pedestrian Navigation </snap>
							<snap class="paperconf">(CGF 2020)</snap> <br>
                            <snap class="author"><snap class="authorme">Kin Chung Kwan</snap>, and Hongbo Fu</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
								In recent years guider-follower approaches show a promising solution to the challenging problem of last-mile or indoor pedestrian navigation without micro-maps or indoor floor plans for path planning. However, the success of such guider-follower approaches is highly dependent on a set of manually and carefully chosen image or video checkpoints. This selection process is tedious and error-prone. To address this issue, we first conduct a pilot study to understand how users as guiders select critical checkpoints from a video recorded while walking along a route, leading to a set of criteria for automatic checkpoint selection. By using these criteria, including visibility, stairs, and clearness, we then implement this automation process. The key behind our technique is a lightweight, effective algorithm using left-hand-side and right-hand-side objects for path occlusion detection, which benefits both automatic checkpoint selection and occlusion-aware path annotation on selected image checkpoints. Our experimental results show that our automatic checkpoint selection method works well in different navigation scenarios. The quality of automatically selected checkpoints is comparable to that of manually selected ones and higher than that of checkpoints by alternative automatic methods.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank" href="files/papers/[2020_CGF]_Automatic_Image_Checkpoint_Selection_for_Guider-Follower_Pedestrian_Navigation.pdf">Paper</a> 
							<a class="better" target="_blank"  href="https://www.youtube.com/watch?v=nQ5RrdtvDiw">Video</a>
							<a class="better" target="_blank"  href=" https://doi.org/10.1111/cgf.14192">DOI</a>
							</div>
                        </td>
                    </tr>    
					<tr>
						
                        <td class="c1"><img class="paperrep" src="img/rep/ardraw_rep.png"/></td>
                        <td class="c2">
							<a class="anchor" id="ardraw_anchor"></a>
                            <snap class="papername">3D Curve Creation on and around Physical Objects with Mobile AR</snap>
							<snap class="paperconf">(TVCG 2020; presented in IEEE VR 2021)</snap> <br>
                            <snap class="author">Hui Ye, <snap class="authorme">Kin Chung Kwan</snap>, and Hongbo Fu</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
								The recent advance in motion tracking (e.g., Visual Inertial Odometry) allows the use of a mobile phone as a 3D pen, thus significantly benefiting various mobile Augmented Reality (AR) applications based on 3D curve creation. However, when creating 3D curves on and around physical objects with mobile AR, tracking might be less robust or even lost due to camera occlusion or textureless scenes. This motivates us to study how to achieve natural interaction with minimum tracking errors during close interaction between a mobile phone and physical objects. To this end, we contribute an elicitation study on input point and phone grip, and a quantitative study on tracking errors. Based on the results, we present a system for direct 3D drawing with an AR-enabled mobile phone as a 3D pen, and interactive correction of 3D curves with tracking errors in mobile AR. We demonstrate the usefulness and effectiveness of our system for two applications: in-situ 3D drawing, and direct 3D measurement.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank" href="files/papers/[2020_TVCG]_3D_Curve_Creation_on_and_around_Physical_Objects_with_Mobile_AR.pdf">Paper</a> 
							<a class="better" target="_blank" href="https://www.youtube.com/watch?v=zyh4pEvK7j8">Video</a>
							<a class="better" target="_blank"  href=" https://doi.org/10.1111/cgf.14192">DOI</a>
							</div>
                        </td>
                    </tr>    
					<tr>
						
                        <td class="c1"><img class="paperrep" src="img/rep/aranimator_rep.png"/></td>
                        <td class="c2">
							<a class="anchor" id="aranimator_anchor"></a>
                            <snap class="papername">ARAnimator: In-situ Character Animation in Mobile AR with User-defined Motion Gestures</snap>
							<snap class="paperconf">(SIGGRAPH 2020)</snap> <br>
                            <snap class="author">Hui Ye, <snap class="authorme">Kin Chung Kwan (Joint first author)</snap>, Wanchao Su, and Hongbo Fu</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
								Creating animated virtual AR characters closely interacting with real environments is interesting but difficult. Existing systems adopt video seethrough approaches to indirectly control a virtual character in mobile AR, making close interaction with real environments not intuitive. In this work we use an AR-enabled mobile device to directly control the position and motion of a virtual character situated in a real environment. We conduct two guessability studies to elicit user-defined motions of a virtual character interacting with real environments, and a set of user-defined motion gestures describing specific character motions. We found that an SVM-based learning approach achieves reasonably high accuracy for gesture classification from the motion data of a mobile device. We present ARAnimator, which allows novice and casual animation users to directly represent a virtual character by an AR-enabled mobile phone and control its animation in AR scenes using motion gestures of the device, followed by animation preview and interactive editing through a video see-through interface. Our experimental results show that with ARAnimator, users are able to easily create in-situ character animations closely interacting with different real environments.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank" href="files/papers/[2020_SIGGRAPH]_ARAnimator_In-situ_Character_Animation_in_Mobile_AR_with_User-defined_Motion_Gestures.pdf">Paper</a> 
							<a class="better" target="_blank" href="https://www.youtube.com/watch?v=neHS18E_3d4">Video</a>
							<a class="better" target="_blank" href="#">Results Video</a>
							<a class="better" target="_blank"  href=" https://doi.org/10.1145/3386569.3392404">DOI</a>
							</div> 
                        </td>
                    </tr>    
                    <tr>
						
                        <td class="c1"><img class="paperrep" src="img/rep/mobi_rep.png"/></td>
                        <td class="c2">
							<a class="anchor" id="mobi_anchor"></a>
                            <snap class="papername">Mobi3DSketch: 3D Sketching in Mobile AR</snap>
							<snap class="paperconf">(CHI 2019)</snap> <br>
                            <snap class="author"><snap class="authorme">Kin Chung Kwan</snap>, and Hongbo Fu</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
                                Mid-air 3D sketching has been mainly explored in Virtual Reality (VR) and typically requires special hardware for motion capture and immersive, stereoscopic displays. The recently developed motion tracking algorithms allow real-time tracking of mobile devices, and have enabled a few mobile applications for 3D sketching in Augmented Reality (AR). However, they are more suitable for making simple drawings only, since they do not consider special challenges with mobile AR 3D sketching, including the lack of stereo display, narrow field of view, and the coupling of 2D input, 3D input and display. To address these issues, we present Mobi3DSketch, which integrates multiple sources of inputs with tools, mainly different versions of 3D snapping and planar/curves surface proxies. Our multimodal interface supports both absolute and relative drawing, allowing easy creation of 3D concept designs in situ. The effectiveness and expressiveness of Mobi3DSketch are demonstrated via a pilot study.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank" href="files/papers/[2019_CHI]_Mobi3DSketch_3D_Sketching_Using_Mobile_AR.pdf">Paper</a> 
							<a class="better" target="_blank" href="https://www.youtube.com/watch?v=JdP0nkeMEog">Video</a>
							<a class="better" target="_blank" href="https://www.youtube.com/watch?v=RSKJdAzqMHo">Video: making process</a>
							<a class="better" target="_blank"  href=" https://doi.org/10.1145/3290605.3300406">DOI</a>
							</div> 
							
							</div>
                        </td>
                    </tr>                    
                    <tr>
                        <td class="c1"><img class="paperrep" src="img/rep/bimanual_rep.png"/></td>
                        <td class="c2">
                            <snap class="papername">Occlusion-robust Bimanual Gesture Recognition by Fusing Multi-views</snap>
							<snap class="paperconf">(Multimedia Tools and Applications 2019)</snap> <br>
                            <snap class="author">Geoffrey Poon, <snap class="authorme">Kin Chung Kwan</snap>, and Wai-Man Pang </snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
                                Human hands are dexterous and always be an intuitive way to instruct or communicate with peers. In recent years, hand gesture is widely used as a novel way for human computer interaction as well. However, existing approaches target solely to recognize single-handed gesture, but not gestures with two hands in close proximity (bimanual gesture). Thus, this paper tries to tackle the problems in bimanual gestures recognition which are not well studied from the literature. To overcome the critical issue of hand-hand self-occlusion problem in bimanual gestures, multiple cameras from different view points are used. A tailored multi-camera system is constructed to acquire multi-views bimanual gesture data. By employing both shape and color features, classifiers are trained with our bimanual gestures dataset. A weighted sum fusion scheme is employed to ensemble results predicted from different classifiers. While, the weightings in the fusion are optimized according to how well the recognition performed on a particular view. Our experiments show that multiple-view results outperform single-view results. The proposed method is especially suitable to interactive multimedia applications, such as our two demo programs: a video game and a sign language learner.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank"  href=" https://doi.org/10.1007/s11042-019-7660-y">DOI</a>
							</div>
                        </td>
                    </tr>                    
                                        <tr>
                        <td class="c1"><img class="paperrep" src="img/rep/bimanual_rep.png"/></td>
                        <td class="c2">
                            <snap class="papername">Real-time Multi-view Bimanual Gesture Recognition</snap>
							<snap class="paperconf">(IEEE ICSIP 2018)</snap> <br>
                            <snap class="author">Geoffrey Poon, <snap class="authorme">Kin Chung Kwan</snap>, and Wai-Man Pang </snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
                                This paper presents a learning-based solution to tackle the real-time gesture recognition of bimanual (two hands) gestures which is not well studied from the literature. To overcome the critical issue of hand-hand self-occlusion problem common in bimanual gestures, multiple cameras from diversified views are used. A tailored multi-camera system is constructed to acquire multi-views bimanual gesture data, and data from each view is then fed into a separate classifier for learning. Thus, to ensemble results from these classifiers, we proposed a weighted sum fusion scheme of results from different classifiers. The weightings are optimized according to how well the recognition performed of the particular view. Our experiments show multiple-view results outperform single-view results.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank"  href=" https://doi.org/10.1109/SIPROCESS.2018.8600529">DOI</a>
							</div>
                        </td>
                    </tr>                    
                    <tr>
						
                        <td class="c1"><img class="paperrep" src="img/rep/decomp_rep.png"/></td>
                        <td class="c2">
							<a class="anchor" id="decomp_anchor"></a>
                            <snap class="papername">Packing Vertex Data into Hardware-decompressible Textures.</snap>
							<snap class="paperconf">(TVCG 2017)</snap> <br>
                            <snap class="author"><snap class="authorme">Kin Chung Kwan</snap>, Xuemiao Xu, Liang Wan, Tien-Tsin Wong, and Wai-Man Pang</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
                                Most graphics hardware features memory to store textures and vertex data for rendering. However, because of the irreversible trend of increasing complexity of scenes, rendering a scene can easily reach the limit of memory resources. Thus, vertex data are preferably compressed, with a requirement that they can be decompressed during rendering. In this paper, we present a novel method to exploit existing hardware texture compression circuits to facilitate the decompression of vertex data in graphics processing unit (GPUs). This built-in hardware allows real-time, random-order decoding of data. However, vertex data must be packed into textures, and careless packing arrangements can easily disrupt data coherence. Hence, we propose an optimization approach for the best vertex data permutation that minimizes compression error. All of these result in fast and high-quality vertex data decompression for real-time rendering. To further improve the visual quality, we introduce vertex clustering to reduce the dynamic range of data during quantization. Our experiments demonstrate the effectiveness of our method for various vertex data of 3D models during rendering with the advantages of a minimized memory footprint and high frame rate.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_black" href="files/papers/[2017_TVCG]_Packing_Vertex_Data_into_Hardware-Decompressible_Textures.pdf">Paper</a>
							<a class="better" target="_blank"  href=" 
							https://doi.org/10.1109/TVCG.2017.2695182">DOI</a>
							
							</div>
                        </td>
                    </tr>                    
                    <tr>
                        <td class="c1"><img class="paperrep" src="img/rep/where2buy_rep.png"/></td>
                        <td class="c2">
                            <snap class="papername">Where2Buy: A Location-Based Shopping App with Products-wise Searching</snap>
							<snap class="paperconf">(IEEE Workshop on Interactive Multimedia Application and Design for Quality Living 2017)</snap> <br>
                            <snap class="author">Kin Chi Chan, Tak Leung Cheung, Siu Hong Lai, <snap class="authorme">Kin Chung Kwan</snap>, Hoyin Yue and Wai-Man Pang</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
                                It is usual for a consumer to search a product based on its category and go to related kind of shop to buy a product, e.g. food in supermarket, a pencil from a stationary shop and etc. While it is not uncommon nowadays for a shop to sell various categories of goods at the same time, like a newspaper stand do sell toys, an accessory shop has stationary. However, consumer may not easily notice and purchase these goods, especially if they are in hurry or not familiar with the shops nearby. With the emergence and popularity of many shopping search engines (shopbots), we can actually provide a better matching between the consumer and seller. In this paper, we developed a shopbot app system (Where2Buy) on smartphone that can search and filter the nearby shops which sell the desired products. To simplify the input process, our system allows users to search by text or voice and fuzzy matching is supported to widen the scope of searching. Detailed information of related product and shops are displayed in the result, together with a navigation map showing the best route to the target shops. If the desired product is not available nearby, substitutes in the same category will be recommended for the users. Our user study evidences that our system is simply and easy to use. More than half of the participants prefer Where2Buy than the other available shopbots in Hong Kong.
                                <br>
                            </div>
                            <div class="paperlink">
								<a class="better" target="_blank"  href=" 
							https://doi.org/10.1109/ISM.2017.87">DOI</a>
							</div>
                        </td>
                    </tr>   
                    <tr>
                        <td class="c1"><img class="paperrep" src="img/rep/childcare_rep.png"/></td>
                        <td class="c2">
                            <snap class="papername">Towards Using Tiny Sensors with Heat Balancing Criteria for Child Care Reminders</snap>
							<snap class="paperconf">(International Journal of Semantic Computing 2016)</snap> <br>
                            <snap class="author">Geoffrey Poon, <snap class="authorme">Kin Chung Kwan</snap>, Wai-Man Pang and Kup-Sze Choi</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
                                Raising children is challenging and requires lots of care. Parents always have to provide proper care to their children in time, like hydration and clothing. However, it is difficult to always stay alert or be aware of the care required at proper moments. One reason is that parents nowadays are busy. This especially applies to single parent, or the one who needs to raise multiple children. This paper presents the use of an integrated multi-sensors together with a mobile application to help keep track of unusual situations concerning a child. By monitoring the changes in surrounding temperature, motions, and air pressure acquired from the sensors, our mobile application can infer the physiological needs of the children with the heat equilibrium assumption. As the thermal environment in the human body is mainly governed by the heat balance equation, we fuse all available sensor readings to the equation so as to estimate the change in situation of a child over a certain period of time. Our system can then notify the parents of the necessary care, including hydration, dining, clothing and ear barotrauma relieving. The proposed application can greatly relieve some of the mental load and pressure of the parents in taking care of children.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank"  href=" 
							https://doi.org/10.1142/S1793351X16400146">DOI</a>
							</div>
                        </td>
                    </tr>     
                    <tr>
                        <td class="c1"><img class="paperrep" src="img/rep/childcare_rep.png"/></td>
                        <td class="c2">
                            <snap class="papername">Towards Using Tiny Multi-sensors Unit for Child Care Reminders</snap>
							<snap class="paperconf">(IEEE BigMM 2016)</snap> <br>
                            <snap class="author">Geoffrey Poon, <snap class="authorme">Kin Chung Kwan</snap>, Wai-Man Pang, and Kup-Sze Choi</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
                                Raising children is challenging and requires lots of care. Parents always have to keep track of the status of their children, and provide proper care to them in time, like hydration, dinning, clothing, discomfort relieving, etc. However, it is always difficult to stay alert or be aware of the care required to the children at proper moments. One reason is that parents nowadays are busy, as they usually have to work outside and lack of skills in looking after their children. This especially applies to case that only one of the parents can take care of the child, or one needs to raise several children at the same time. This paper presents the use of an integrated multi-sensors unit together with a mobile application to help keeping track of unusual situations of a child. By monitoring the changes in the surrounding temperature, motions, and air pressure acquired from the sensors, our mobile application can detect the status of the children and notify the parents the necessary care to them. The care includes hydration, dinning, clothing and ear barotrauma relieving. The proposed mobile application can greatly relieve some of the mental load and pressure of the parents in taking care of children.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank"  href=" 
							https://doi.org/10.1109/BigMM.2016.52">DOI</a>
							</div>
                        </td>
                    </tr>
                                        <tr>
                        <td class="c1"><img class="paperrep" src="img/rep/lunar_rep.png"/></td>
                        <td class="c2">
                            <snap class="papername">A Two-Phase Space Resection Model for Accurate Topographic Reconstruction from Lunar Imagery with Pushbroom Scanners</snap>
							<snap class="paperconf">(Sensors 2016)</snap> <br>
                            <snap class="author">Xuemiao Xu, Huaidong Zhang,Guoqiang Han, <snap class="authorme">Kin Chung Kwan</snap>, Wai-Man Pang, Jiaming Fang, and Gansen Zhao </snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
                                Exterior orientation parameters’ (EOP) estimation using space resection plays an important role in topographic reconstruction for push broom scanners. However, existing models of space resection are highly sensitive to errors in data. Unfortunately, for lunar imagery, the altitude data at the ground control points (GCPs) for space resection are error-prone. Thus, existing models fail to produce reliable EOPs. Motivated by a finding that for push broom scanners, angular rotations of EOPs can be estimated independent of the altitude data and only involving the geographic data at the GCPs, which are already provided, hence, we divide the modeling of space resection into two phases. Firstly, we estimate the angular rotations based on the reliable geographic data using our proposed mathematical model. Then, with the accurate angular rotations, the collinear equations for space resection are simplified into a linear problem, and the global optimal solution for the spatial position of EOPs can always be achieved. Moreover, a certainty term is integrated to penalize the unreliable altitude data for increasing the error tolerance. Experimental results evidence that our model can obtain more accurate EOPs and topographic maps not only for the simulated data, but also for the real data from Chang’E-1, compared to the existing space resection model.
                                <br>
                            </div>
                            <div class="paperlink">
								<a class="better" target="_blank" href="https://doi.org/10.3390/s16040507">DOI</a>
							</div>
                        </td>
                    </tr>                    
                    <tr>
                        <td class="c1"><img class="paperrep" src="img/rep/food_rep.png"/></td>
                        <td class="c2">
                            <snap class="papername">A Mobile Adviser of Healthy Eating by Reading Ingredient Labels</snap>
							<snap class="paperconf">(MOBIHEALTH 2016)</snap> <br>
                            <snap class="author">Man Wai Wong, Qing Ye, Yuk Kai Chan Kylar, Wai-Man Pang, and <snap class="authorme">Kin Chung Kwan</snap></snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
                                Understanding ingredients or additives in food is essential for a healthy life. The general public should be encouraged to learn more about the effect of food they consume, especially for people with allergy or other health problems. However, reading the ingredient label of every packaged food is tedious and no one will spend time on this. This paper proposed a mobile app to leverage the troublesome and at the same time provide health advices of packaged food. To facilitate acquisition of the ingredient list, apart from barcode scanning, we recognize text on the ingredient labels directly. Thus, our application will provide proper alert on allergen found in food. Also it suggests users to avoid food that is harmful to health in long term, like high fat or calories food. A preliminary user study reveals that the adviser app is useful and welcomed by many users who care about their dietary.
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" target="_blank" href="files/papers/[2016_MOBIHEALTH]_A_Mobile_Adviser_of_Healthy_Eating_by_Reading_Ingredient_Labels.pdf">Paper</a>
							<a class="better" target="_blank" href="https://doi.org/10.1007/978-3-319-58877-3_4" target="_blank">DOI</a>
							</div>
                        </td>
                    </tr>                    
                    <tr>
						
                        <td class="c1"><img class="paperrep" src="img/rep/pad_rep.png"/></td>
                        <td class="c2">
							<a class="anchor" id="pad_anchor"></a>
                            <snap class="papername">Pyramid of Arclength Descriptor for Generating Collage of Shapes</snap>
							<snap class="paperconf">(SIGGRAPH Asia 2016)</snap> <br>
                            <snap class="author"><snap class="authorme">Kin Chung Kwan, </snap> Lok-Tsun Sinn Jimmy, Chu Han, Tien-Tsin Wong, and Chi-Wing Fu</snap> <br>
                            <div class="paperabs">
                                <snap class="paperabsicon">Abstract</snap>
                                This paper tackles a challenging 2D collage generation problem, focusing on shapes: we aim to fill a given region by packing irregular and reasonably-sized shapes with minimized gaps and overlaps. To achieve this nontrivial problem, we first have to analyze the boundary of individual shapes and then couple the shapes with partially-matched boundary to reduce gaps and overlaps in the collages. Second, the search space in identifying a good coupling of shapes is highly enormous, since arranging a shape in a collage involves a position, an orientation, and a scale factor. Yet, this matching step needs to be performed for every single shape when we pack it into a collage. Existing shape descriptors are simply infeasible for computation in a reasonable amount of time. To overcome this, we present a brand new, scale- and rotation-invariant 2D shape descriptor, namely pyramid of arclength descriptor (PAD). Its formulation is locally supported, scalable, and yet simple to construct and compute. These properties make PAD efficient for performing the partial-shape matching. Hence, we can prune away most search space with simple calculation, and efficiently identify candidate shapes. We evaluate our method using a large variety of shapes with different types and contours. Convincing collage results in terms of visual quality and time performance are obtained. 
                                <br>
                            </div>
                            <div class="paperlink">
							<a class="better" href="files/papers/[2016_SIGGRAPH_Asia]_Pyramid_of_Arclength_Descriptor_for_Collage_of_Shapes.pdf" target="_blank">Paper</a>
							<a class="better" href="http://appsrv.cse.cuhk.edu.hk/~ttwong/cgi-bin/paper-download/download.cgi?path=pad&dl=pad_supp.pdf" target="_blank">Supp</a>
							<a class="better" href="https://youtu.be/_q0l_CcwBJ4">Video</a>
							<a class="better" href="https://youtu.be/mLzzwZf6p6s">Video: Making Processes</a>
							<a class="better" href="https://doi.org/10.1145/2980179.2980234" target="_blank">DOI</a>
							</div>
							
                        </td>
                    </tr>                    
                </table>
                
            </div>
			<a class="anchor" id="activity_anchor"></a>
            <h1 id="activity"><span> Activities </span></h1>
			<div class="section">
			
				<h2>Teaching Activites</h2>
					Teacher
					<table class="teachingtable">
						<tr>
							<td class="c1"><span class="newdate3"> 2021 Winter</span></td>
							<td class="c2"> University of Konstanz</td>
							<td class="c3"> Illustrative Computer Graphics</td>
						</tr>
						<tr>
							<td class="c1"><span class="newdate3"> 2015 Summer</span></td>
							<td class="c2"> The Hong Kong Jockey Club</td>
							<td class="c3"> CUDA Training</td>
						</tr>
					</table>
					<br><br>
					Teaching Assistants
					<table class="teachingtable">
					<tr>
                        <td class="c1"><span class="newdate3">2020 - 2021</span></td>
                        <td class="c2">University of Konstanz</td>
						<td class="c3">  Seminar: Current Trends in Computer Graphics </td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate3"> 2015 - 2018  </span></td>
                        <td class="c2">The Chinese Univeristy of Hong Kong</td>
						<td class="c3">  CMSC5716 Web-Based Graphics and Virtual Reality</td>
                    </tr>
					
					<tr>
                        <td class="c1"><span class="newdate3"> 2014 - 2017 </span></td>
                        <td class="c2">The Chinese Univeristy of Hong Kong</td>
						<td class="c3">  CMSC5736 Mobile Apps Design and Implementation</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate3"> 2014 Summer </span></td>
                        <td class="c2">The Chinese Univeristy of Hong Kong</td>
						<td class="c3">  CMSC5714 Multimedia Technology</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate3"> 2013 - 2019 </span></td>
                        <td class="c2">The Chinese Univeristy of Hong Kong</td>
						<td class="c3">  CMSC5727 Computer Game Software Production</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate3"> 2011 - 2012 </span></td>
                        <td class="c2">The Chinese Univeristy of Hong Kong </td>
						<td class="c3">  CSC5390 Advanced GPU Programming</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate3"> 2010 Winter </span></td>
                        <td class="c2">The Chinese Univeristy of Hong Kong </td>
						<td class="c3">  CSC4120 Principles of Computer Game Software </td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate3"> 2010 - 2011 </span></td>
                        <td class="c2">The Chinese Univeristy of Hong Kong </td>
						<td class="c3">  CSC3280 Introduction to Multimedia Systems</td>
                    </tr>
					<tr>
                        <td class="c1"><span class="newdate3"> 2009 Winter</span></td>
                        <td class="c2">The Chinese Univeristy of Hong Kong </td>
						<td class="c3">  CSC1130 Introduction to Computing Using Java </td>
                    </tr>
                </table>
                <div style="height:20pt"></div>
				<h2>Other Activites</h2>
				<div class="flex-container">
					<div class="flex-childTextA" style="flex-basis:30%;">
						<h2> 2019  </h2>
						<h2> China Night</h2>
						A gathering for Chinese CHI researchers during SIGCHI 2019 in the Rotunda Bar and Diner, Glasgow, United Kingdom.
					</div> 
					<div><img style="width:100%" src="img/photo/chinaNight.jpg"/></div> 
				</div>
				<div class="flex-container">
					<div><img style="width:100%" src="img/photo/pg2018.jpg"/></div> 
					<div class="flex-childTextB" style="flex-basis:50%;">
						<h2> 2018 </h2>
						<h2> Pacific Graphics 2018 Helper  </h2>
						I was a helper in pacific graphics 2018. This photo was captured during the banquet of PG2018 in the Royal Plaza Hotel, Hong Kong.
					</div> 
				</div>
				<div class="flex-container">
					<div class="flex-childTextA" style="flex-basis:60%;">
						<h2> 2016 </h2>
						<h2> First Presentation</h2>
						This is my first conference presentation for my first ACM SIGGRAPH paper. Prof. Philip Fu captured this photo for me. Thank you.
					</div> 
					<div><img style="width:100%" src="img/photo/first.jpg"/></div> 
				</div>
			</div>
			
			<!--<a class="anchor" id="hobby_anchor"></a>
            <h1 id="hobby"><span> Hobby </span></h1>
            <div class="section">
                <!--<h2>Travelling to Japan</h2>
                I like to travel to Japan with my family. Here are some photos captured in Japan using my iPhone camera.<BR>
                <div class="gallay">
                <img src=".jpg"/>
                <img src=".jpg"/>
                </div>
            </div>-->
			<!--<a class="anchor" id="contact_anchor"></a>
            <h1 id="contact"><span> Contact</span></h1>-->
			<!--<a class="anchor" id="resources_anchor"></a>
            <h1 id="resources"><span> Resources </span></h1>
			<div class="section">
				<a href="resources/resources.html">
					<div class="linkbox">
						<h2> Useful Resources </h2>
					</div> 
				</a>
				<a href="tutorial/tutorial.html">
					<div class="linkbox">
						<h2> KC's Learning Notes </h2>
					</div> 
				</a>
            </div>-->
			
			<a class="anchor" id="link_anchor"></a>
			<h1 id="links"><span> Links</span></h1>
			<div class="section">
				<a href="https://tmak.info/">
					<div class="linkbox">
						<h2> My Friend: Terrence Mak's Personal Webpage.</h2>
						https://tmak.info/ <br>
					</div> 
				</a>
				<a href="https://www.overleaf.com/">
					<div class="linkbox">
						<h2> Overleaf  </h2>
						https://www.overleaf.com/ <br>
					</div> 
				</a>
				<a href="http://kesen.realtimerendering.com/">
					<div class="linkbox">
						<h2> Ke-Sen Huang's Home Page  </h2>
						http://kesen.realtimerendering.com/ <br>
					</div> 
				</a>
			</div>
                
            
        </div> 
		<div style="height:500pt"></div>
    </body>
    <footer>
	KC Kwan's webpage @ 2021. All rights reserved.
	</footer>
</html>